{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain DataCamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\luisl\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "data = np.array([0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
      "x = data[:,0]\n",
      "y = data[:,1]\n",
      "\n",
      "plt.bar(x, y)\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint \n",
    "\n",
    "# Set your Hugging Face API token \n",
    "huggingfacehub_api_token = os.getenv(\"HUGGINGFACE_key\")\n",
    "\n",
    "# Define the LLM\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = \"\"\"Wrte a python code to generate a barplot with x='Date' and y='cost'\"\"\"\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".” – George Carlin\n",
      "\n",
      "Shoes are so much more than mere footwear – they can be a reflection of one’s personality, style, and even priorities. But beyond making a fashion statement, taking care of your shoes is essential for their longevity and your comfort. Here are a few tips on how to properly care for your shoes:\n",
      "\n",
      "1. Invest in quality shoes: The first step in taking care of your shoes is to invest in quality footwear. Quality shoes are typically made with better materials and construction, which means they will last longer and withstand wear and tear better than cheap, poorly made shoes. This is especially important for shoes that you wear frequently, such as work or exercise shoes.\n",
      "\n",
      "2. Rotate your shoes: Wearing the same pair of shoes every day can cause them to wear out faster. It’s important to rotate your shoes and give them a break between wears. This allows them to air out and regain their shape, preventing them from becoming smelly or worn out.\n",
      "\n",
      "3. Clean and polish regularly: Regular cleaning and polishing help maintain the appearance of your shoes and prevent them from cracking or drying out. Use a soft brush or cloth to remove any dirt or dust, and then apply a shoe polish that matches the color of your shoes. This will not only keep\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# Define the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates are recipes for generating prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\luisl\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceEndpoint \n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Set your Hugging Face API token \n",
    "huggingfacehub_api_token = os.getenv(\"HUGGINGFACE_key\")\n",
    "\n",
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template = template, input_variables=['question'])\n",
    "\n",
    "# Define the LLM\n",
    "\n",
    "# Set your Hugging Face API token \n",
    "huggingfacehub_api_token = os.getenv(\"HUGGINGFACE_key\")\n",
    "\n",
    "# Define the LLM\n",
    "llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# question = \"How does LangChain make LLM application development easier?\"\n",
    "# print(llm_chain.run(question))\n",
    "\n",
    "### NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To retain learning effectively, you can try the following strategies:\n",
      "\n",
      "1. Review and practice regularly: Consistent review of the material you have learned helps reinforce your memory and understanding.\n",
      "\n",
      "2. Teach others: Explaining concepts to someone else can help solidify your own understanding and retention.\n",
      "\n",
      "3. Use mnemonic devices: Mnemonics, such as acronyms or visual aids, can help you remember information more easily.\n",
      "\n",
      "4. Apply what you've learned: Putting your knowledge into practice through real-life scenarios or problem-solving can enhance retention.\n",
      "\n",
      "5. Stay engaged and curious: Maintain an active interest in the subject matter to keep your brain engaged and motivated to learn.\n",
      "\n",
      "6. Get enough rest and exercise: A healthy lifestyle, including sufficient sleep and physical activity, can support cognitive function and memory retention.\n",
      "\n",
      "By incorporating these strategies into your learning routine, you can improve your ability to retain information effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key= os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Insert a question into the template and call the model\n",
    "full_prompt = prompt_template.format_messages(question='How can I retain learning?')\n",
    "\n",
    "output = llm(full_prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hi! Ask me anything about LangChain.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Describe a metaphor for learning LangChain in one sentence.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hi! Ask me anything about LangChain.\")\n",
    "history.add_user_message(\"Describe a metaphor for learning LangChain in one sentence.\")\n",
    "(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hi! Ask me anything about LangChain.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Describe a metaphor for learning LangChain in one sentence.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Summarize the preceding sentence in fewer words', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_user_message(\"Summarize the preceding sentence in fewer words\")\n",
    "(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\nllm\n  Can't instantiate abstract class BaseLanguageModel without an implementation for abstract methods 'agenerate_prompt', 'apredict', 'apredict_messages', 'generate_prompt', 'invoke', 'predict', 'predict_messages' (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m chat \u001b[38;5;241m=\u001b[39m OpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[0;32m      6\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m buffer_chain \u001b[38;5;241m=\u001b[39m \u001b[43mConversationChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m buffer_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe a language model in one sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m buffer_chain\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe it again using less words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luisl\\anaconda3\\envs\\llms_env\\Lib\\site-packages\\langchain\\load\\serializable.py:75\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\luisl\\anaconda3\\envs\\llms_env\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ConversationChain\nllm\n  Can't instantiate abstract class BaseLanguageModel without an implementation for abstract methods 'agenerate_prompt', 'apredict', 'apredict_messages', 'generate_prompt', 'invoke', 'predict', 'predict_messages' (type=type_error)"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "memory = ConversationBufferMemory(size=4)\n",
    "\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "buffer_chain.predict(input=\"Describe a language model in one sentence\")\n",
    "buffer_chain.predict(input=\"Describe it again using less words\")\n",
    "buffer_chain.predict(input=\"Describe it again fewer words but at least one word\")\n",
    "buffer_chain.predict(input=\"What did I first ask you? I forgot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing External Data for Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangChain Expression Language (LCEL), Chains, and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tools, Troubleshooting, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
